# Intro Scalable Deep Learning: Day 2, Tutorial 1
# Deep Learning Basics Recap

**Content creators**: Roshni Kamath, Jenia Jitsev

**Content reviewers / testers**: Helmholtz AI Team

**Content supervisors** : Jenia Jitsev

# Usage
Please login for the [Tutorial](Day2_Tutorial1_DL_Basics_Recap.ipynb) into [Jupyter-JSC](https://jupyter-jsc.fz-juelich.de/), choosing JUWELS, `booster` partition and training account, and execute the notebook in the browser environment once it is started.

# Tutorial Objectives

In the [Tutorial](Day2_Tutorial1_DL_Basics_Recap.ipynb), we'll go through some machine learning and deep learning basics. Many of these basic topics will of course remain important when going to larger scale with distributed training.

We will refresh basics on

- train test split, train test error
- data normalization
- optimization via gradient descent
- learning rate / batch size

We will have hands on with:

* MNIST, CIFAR-10 datasets
* Simple fully connected and convolutional networks
* Finding proper learning rate and batch size for training procedure
* Simple learning rate schedulers

The goal is to get reminded what is important for a basic training procedure when dealing with deep neural networks.

This tutorial will take up about 1 hour to complete
