{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scalable DL: Day 3, Tutorial 2\n",
    "\n",
    "**Content creators**: Jan Ebert\n",
    "\n",
    "**Content reviewers / testers**: Stefan Kesselheim, Alexandre Strube\n",
    "\n",
    "**Content supervisors** : Stefan Kesselheim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce some options to fix file system performance and take a more in-depth look at `tf.data` pipelines and their performance in various settings.\n",
    "\n",
    "We have learned that some file systems – such as the one the Jülich supercomputers use – are bad at handling many small files and suffer in performance accordingly.\\\n",
    "Assuming our dataset has many small files and is affected by this degradation, we have multiple solutions available to tackle the performance problem. The main factor is whether our dataset fits into working memory (RAM) or not. We will try out two solutions in this tutorial: one for smaller datasets and one for larger datasets.\n",
    "\n",
    "To get a feel for how some of the functions of `tf.data` work, you will first inspect a simple, synthetic dataset and try out some transformations on it.\\\n",
    "After that, you will work on many files stored on disk and try to optimize the pipeline's throughput performance using various functions and flags available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "# Disable TensorFlow GPU usage so this works on login nodes.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "\n",
    "from tiny_imagenet_tfrecords import from_tfrecords\n",
    "from dataset_wrapper import TFDataset\n",
    "\n",
    "\n",
    "def display_image(image_data):\n",
    "    display(Image.fromarray(image_data.numpy()))\n",
    "\n",
    "\n",
    "def decode_jpeg(image_string):\n",
    "    return tf.io.decode_jpeg(image_string, channels=3)\n",
    "\n",
    "\n",
    "class TFDatasetProfilingResults:\n",
    "    \"\"\"Collect profiling results and nicely format them.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _escape_html(text):\n",
    "        text = text.replace('&', '&amp;')\n",
    "        text = text.replace('<', '&lt;')\n",
    "        text = text.replace('>', '&gt;')\n",
    "        text = text.replace('\"', '&quot;')\n",
    "        text = text.replace(\"'\", '&#39;')\n",
    "        return text\n",
    "\n",
    "    def add_result(self, dataset_set, dataset, duration):\n",
    "        \"\"\"Add a profiling result with `duration` for the given\n",
    "        `dataset` on `dataset_set`.\n",
    "        \"\"\"\n",
    "        self.results.append((\n",
    "            dataset_set,\n",
    "            dataset,\n",
    "            duration,\n",
    "            dataset.num_iterations,\n",
    "        ))\n",
    "        \n",
    "    def to_html_table(self):\n",
    "        \"\"\"Return the collected results formatted as a HTML table.\"\"\"\n",
    "        html_results = ['<table><thead><tr>'\n",
    "                '<th style=\"text-align: left\">Dataset</th>'\n",
    "                '<th style=\"text-align: left\">Pipeline</th>'\n",
    "                '<th>Duration [sec]</th>'\n",
    "                '<th>Iteration</th>'\n",
    "                '</tr></thead><tbody>']\n",
    "\n",
    "        for (dataset_set, dataset, duration, num_iters) in self.results:\n",
    "            pipeline = ',<br>'.join(map(self._escape_html, dataset.pipeline))\n",
    "            html_results.append(\n",
    "                f'<tr>'\n",
    "                f'<td style=\"text-align: left;\">{self._escape_html(dataset_set)}</td>'\n",
    "                f'<td style=\"text-align: left;\">{pipeline}</td>'\n",
    "                f'<td>{duration:.3f}</td>'\n",
    "                f'<td>{num_iters}</td>'\n",
    "                f'</tr>',\n",
    "            )\n",
    "\n",
    "        html_results.append('</tbody></table>')\n",
    "        return ''.join(html_results)\n",
    "    \n",
    "    def averages_to_html_table(self, num_warmup_iterations):\n",
    "        \"\"\"Return averaged results formatted as a HTML table.\n",
    "        Results are only considered after the given number of\n",
    "        warmup iterations `num_warmup_iterations`.\n",
    "        \"\"\"\n",
    "        html_results = ['<table><thead><tr>'\n",
    "                        '<th style=\"text-align: left\">Dataset</th>'\n",
    "                        '<th style=\"text-align: left\">Pipeline</th>'\n",
    "                        '<th>Average Duration [sec]</th>'\n",
    "                        '<th>Samples</th>'\n",
    "                        '</tr></thead><tbody>']\n",
    "\n",
    "        pipelines = {}\n",
    "        for (dataset_set, dataset, duration, num_iters) in self.results:\n",
    "            if num_iters <= num_warmup_iterations:\n",
    "                continue\n",
    "\n",
    "            key = (dataset_set, tuple(dataset.pipeline))\n",
    "            durations = pipelines.setdefault(key, [])\n",
    "            durations.append(duration)\n",
    "\n",
    "        for ((dataset_set, pipeline), durations) in pipelines.items():\n",
    "            num_samples = len(durations)\n",
    "            avg_duration = sum(durations) / num_samples\n",
    "            pipeline = ',<br>'.join(map(self._escape_html, pipeline))\n",
    "            html_results.append(\n",
    "                f'<tr>'\n",
    "                f'<td style=\"text-align: left;\">{self._escape_html(dataset_set)}</td>'\n",
    "                f'<td style=\"text-align: left;\">{pipeline}</td>'\n",
    "                f'<td>{avg_duration:.3f}</td>'\n",
    "                f'<td>{num_samples}</td>'\n",
    "                f'</tr>',\n",
    "            )\n",
    "\n",
    "        html_results.append('</tbody></table>')\n",
    "        if len(html_results) <= 2:\n",
    "            html_results.append('<i>No run had enough iterations!</i>')\n",
    "        return ''.join(html_results)\n",
    "    \n",
    "    def to_csv(self, separator=';'):\n",
    "        \"\"\"Return the collected results in CSV format with the\n",
    "        given `separator`.\n",
    "        \"\"\"\n",
    "        csv_results = [\n",
    "            separator.join(['dataset', 'pipeline', 'duration', 'num_iterations']),\n",
    "        ]\n",
    "\n",
    "        for (dataset_set, dataset, duration, num_iters) in self.results:\n",
    "            pipeline = ','.join(dataset.pipeline)\n",
    "            csv_results.append(separator.join(map(str,\n",
    "                (dataset_set, pipeline, duration, num_iters),\n",
    "            )))\n",
    "\n",
    "        return '\\n'.join(csv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Datasets with Many Files\n",
    "\n",
    "### This section of the tutorial is very Linux-specific and will not work on Windows machines!\n",
    "\n",
    "Whenever we have to work with many individual files on the Jülich supercomputers, we are likely to encounter runtime performance degradations. For data-intensive work like deep learning, this loss in performance is usually unacceptable and we need to adapt our dataset to work with our file system.\n",
    "\n",
    "Usually, this entails packing the dataset into a single file with whatever means necessary. However, doing this means that we would also have to change our program's code to read data from this new, single file instead. Ideally, we would like to keep our code the same while still avoiding the file system issues. We'll take a look at two solutions: the first uses the temporary [_shared memory_ drive `/dev/shm`](https://en.wikipedia.org/wiki/Shared_memory#Support_on_Unix-like_systems) to make in-memory data available to the whole node. The other solution uses [SquashFS](https://en.wikipedia.org/wiki/SquashFS) combined with a virtual file system (a [filesystem in userspace (FUSE)](https://en.wikipedia.org/wiki/Filesystem_in_Userspace) via [`squashfuse`](https://github.com/vasi/squashfuse)).\n",
    "\n",
    "Let's assume we have two datasets that consist of many files.\n",
    "1. One of them (_dataset A_) is relatively small (< 500 GB) and fits into the working memory (RAM) of each compute node (together with our program state).\n",
    "2. The other dataset (_dataset B_) is larger (> 500 GB) and does not fit into RAM.\n",
    "\n",
    "Depending on whether our dataset fits in RAM, we can adapt our handling accordingly. For the smaller dataset, this will benefit us in two ways: more speed and simplicity.\n",
    "\n",
    "Let's first take a look at how to handle _dataset A_, which fits into memory. This uses the temporary file system at `/dev/shm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving to an In-memory Dataset\n",
    "\n",
    "If our data fits into memory, the simplest option on a Linux system (like the Jülich supercomputers) is to simply copy it into `/dev/shm` and read from there. In practical terms, we move our data from disk into RAM. We need to be careful to copy the data _exactly once_ on each node, because **all processes on the same node** can read from and write to the same `/dev/shm`, but processes on different nodes see a different `/dev/shm` – `/dev/shm` is node-local storage. If we were to copy the data multiple times in parallel, it will likely get corrupted; if we only copied the data once on the launch node, any other node would not be able to find the data because it doesn't exist on that node.\n",
    "\n",
    "Another thing to worry about with this approach: `/dev/shm` is a temporary file system and changes on it will not be reflected later nor accessible from other nodes. If you plan to write data, it is easiest to do it in a location other than `/dev/shm`.\n",
    "\n",
    "It may be slow to copy a bunch of files from the file system since you run into the same problem as always – accessing individual files causes performance problems. Even though we only do this once at the start of the job, the time may pile up. To alleviate this slightly, an easy option is to create a `tar` archive of the data, which is then extracted right from the single archive file into `/dev/shm`.\n",
    "\n",
    "A template that you can adapt to your own jobs can be found in `shm.sbatch`. In there, the data is automatically `tar`ed once and then `untar`ed into `/dev/shm`.\n",
    "\n",
    "Let's try out the technique on the Tiny-ImageNet-200 dataset. In Python (and for future usage in this notebook), we can do the following, assuming we are not running multiple Python processes in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "data_path = Path('/p/project/training2306/datasets/tiny-imagenet-200').expanduser()\n",
    "tar_path = data_path.with_suffix('.tar')\n",
    "shm_path = Path(f'/dev/shm/{getpass.getuser()}/{data_path.name}')\n",
    "\n",
    "# Create a tar archive of the data if it doesn't exist.\n",
    "if not tar_path.exists():\n",
    "    with tarfile.open(tar_path, 'w') as tar:\n",
    "        tar.add(data_path, arcname=data_path.name)\n",
    "\n",
    "# Clean up any remains of previous jobs.\n",
    "shutil.rmtree(shm_path, ignore_errors=True)\n",
    "shm_path.mkdir(parents=True)\n",
    "\n",
    "# Extract the tar file into `shm_path`.\n",
    "with tarfile.open(tar_path, 'r') as tar:\n",
    "    tar.extractall(shm_path.parent)\n",
    "# Make our data private so future jobs of other users will not be able\n",
    "# to access possibly sensitive data in this shared location.\n",
    "shm_path.chmod(0o700)\n",
    "\n",
    "# Read data from `shm_path`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving to a FUSE-mounted SquashFS\n",
    "\n",
    "For data that does not fit into memory, we can pack all data into a SquashFS. This SquashFS is a compressed, read-only archive of the data. That means if our data changes, we have to re-create the SquashFS every time. When your data is very dynamic, this may become annoying.\n",
    "\n",
    "After creating the SquashFS, we are left with a single file that contains all our data; reading from this would be different than reading from the file system, meaning we'd have to re-write our data reading code. Thankfully, the `squashfuse_ll` binary allows us to access the SquashFS contents like a standard directory. `squashfuse_ll` will _mount_ the SquashFS to a different path and make it look like any other file system path. The FUSE mount is like a link to the SquashFS, only that it presents the data transparently as a standard file system. Once again, we avoid changing the code that is responsible for reading data, but retain the speed benefits of a single-file dataset. One thing we should always take care of, though, is cleaning up the mount directory afterwards, so we do not pollute resources. This is achieved by unconditionally calling `fusermount3 -u <mount_dir>` when the script exits.\n",
    "\n",
    "The same caveats as for the in-memory usage of `/dev/shm` apply: first off, make sure that we only create, mount, and un-mount once per node. Also, mount locations are limited: on Jülich machines, you have to use directories in `/dev/shm` or `/tmp`. Finally, it's important to mention again that SquashFS files are _read-only_, so we cannot write to the mount path.\n",
    "\n",
    "Even if we mount the data in `/dev/shm`, remember that the mount is like a link – the data does not actually reside in RAM, unlike in the previous section.\n",
    "\n",
    "To summarize the basic steps for using a SquashFS are:\n",
    "1. Create the SquashFS using `mksquash`.\n",
    "2. Mount the SquashFS to a directory in `/dev/shm` using `squashfuse_ll`.\n",
    "3. Do your work...\n",
    "4. Unmount the SquashFS mount directory using `fusermount -u`.\n",
    "\n",
    "For a template that you can adapt to your own jobs, see `squashfs.sbatch`. Note that the template is much more complex than what is described here due to having to handle creation and clean-up on multiple nodes in parallel while also working around Slurm limitations. We hope that the extensive documentation in the template can clear up what is happening.\n",
    "\n",
    "Let's create and mount a SquashFS of Tiny-ImageNet-200 right here in Python, assuming we have only one Python process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atexit\n",
    "import getpass\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "data_path = Path('/p/project/training2306/datasets/tiny-imagenet-200').expanduser()\n",
    "sqsh_path = Path(f'{data_path}.sqsh')\n",
    "mount_path = Path(f'/dev/shm/{getpass.getuser()}/sqsh/{data_path.name}')\n",
    "\n",
    "# Create a SquashFS of the data if it doesn't exist.\n",
    "if not sqsh_path.exists():\n",
    "    subprocess.run(['mksquashfs', data_path, sqsh_path], check=True)\n",
    "\n",
    "def clean_up_squashfuse(mount_path):\n",
    "    if mount_path.is_dir():\n",
    "        subprocess.run(['fusermount3', '-u', mount_path])\n",
    "    shutil.rmtree(mount_path, ignore_errors=True)\n",
    "\n",
    "# Clean up any remains of previous jobs.\n",
    "clean_up_squashfuse(mount_path)\n",
    "# Create our mount directory.\n",
    "mount_path.mkdir(mode=0o700, parents=True, exist_ok=True)\n",
    "\n",
    "# Unmount the SquashFS when Python exits.\n",
    "atexit.register(clean_up_squashfuse, mount_path)\n",
    "# Mount the SquashFS at our mount path.\n",
    "subprocess.run(['squashfuse_ll', sqsh_path, mount_path], check=True);\n",
    "\n",
    "# Read data from `mount_path`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Gains?\n",
    "\n",
    "Here we will compare how the methods of\n",
    "1. loading from the standard file system,\n",
    "2. loading data directly inside `/dev/shm`, and\n",
    "3. loading from a FUSE-mounted SquashFS\n",
    "\n",
    "stack up against each other. Remember that the only thing we change is the path we read the data from – everything else is exactly the same across the three methods.\n",
    "\n",
    "What do you expect will be fastest on the Jülich machines? What will be slowest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_dataset(dataset_path):\n",
    "    assert dataset_path.exists(), 'dataset directory does not exist'\n",
    "    print('Reading from', dataset_path)\n",
    "\n",
    "    # Start a timer so we can measure starting from dataset creation.\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Create a generator containing all JPEG images in the dataset.\n",
    "    files = dataset_path.glob('**/*.JPEG')\n",
    "    files = list(files)\n",
    "\n",
    "    iteration_start_time = time.perf_counter()\n",
    "\n",
    "    # Iterate through the dataset; just access every file once.\n",
    "    for filename in files:\n",
    "        # We try to avoid file system caching with this call.\n",
    "        filename.stat()\n",
    "        with filename.open('rb') as f:\n",
    "            f.read()\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    duration = end_time - start_time\n",
    "    iteration_duration = end_time - iteration_start_time\n",
    "\n",
    "    print('Total duration:', duration, 'seconds')\n",
    "    print('Iteration duration:', iteration_duration, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dataset(shm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mount_path.exists(), 'dataset directory does not exist'\n",
    "assert (mount_path / 'val').exists(), 'dataset directory is not mounted'\n",
    "\n",
    "profile_dataset(mount_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "On the Jülich machines, the ranking should look like this:\n",
    "1. Data directly in `/dev/shm`\n",
    "2. SquashFS + FUSE\n",
    "3. Standard file system.\n",
    "\n",
    "Between these options, you should always prefer `/dev/shm` since it will be super fast. However, due to the caveat of the dataset having to fit into memory together with the program state, `/dev/shm` cannot always be used. In that case and if your data does not change very often, using a SquashFS will usually yield better performance compared to the standard file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to Know `tf.data`\n",
    "\n",
    "### From here on out, the tutorial is Windows-compatible again!\n",
    "\n",
    "You will now be able to play around with some `tf.data` pipelines in various settings. First use the provided `synthetic_data` generator or write your own. Get a feel for how the `shard` and `batch` methods work. Can you guess what is the difference between `dataset.shard(2, 0).batch(2)` and  `dataset.batch(2).shard(2, 0)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharded: [0, 2, 4, 6, 8, 10, 12, 14]\n",
      "Batched:\n",
      "[[0 1],\n",
      " [2 3],\n",
      " [4 5],\n",
      " [6 7],\n",
      " [8 9],\n",
      " [10 11],\n",
      " [12 13],\n",
      " [14 15]]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_data():\n",
    "    for i in range(16):\n",
    "        yield i\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    synthetic_data,\n",
    "    output_signature=tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    ")\n",
    "\n",
    "sharded = dataset.shard(2, 0)\n",
    "print('Sharded:', [d.numpy() for d in sharded])\n",
    "\n",
    "batched = dataset.batch(2)\n",
    "print('Batched:\\n[' + ',\\n '.join([str(d.numpy()) for d in batched]) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache and Prefetch\n",
    "Now, the dataset will emulate a short loading time and print out which element is generated when. Please try to understand the code examples and the output they produce. Afterwards, you can create similar examples for [other methods in the module](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\\\n",
    "There are some task suggestions at the bottom of this subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First read of cached dataset\n",
      "Yielding element 0\n",
      "Getting element tf.Tensor(0, shape=(), dtype=int32)\n",
      "Yielding element 1\n",
      "Getting element tf.Tensor(1, shape=(), dtype=int32)\n",
      "Yielding element 2\n",
      "Getting element tf.Tensor(2, shape=(), dtype=int32)\n",
      "Yielding element 3\n",
      "Getting element tf.Tensor(3, shape=(), dtype=int32)\n",
      "Yielding element 4\n",
      "Getting element tf.Tensor(4, shape=(), dtype=int32)\n",
      "Yielding element 5\n",
      "Getting element tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def sleepy_synthetic_data():\n",
    "    for i in range(6):\n",
    "        time.sleep(0.5)\n",
    "        print('Yielding element', i)\n",
    "        yield i\n",
    "\n",
    "sleepy_dataset = tf.data.Dataset.from_generator(\n",
    "    sleepy_synthetic_data,\n",
    "    output_signature=tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    ")\n",
    "cached = sleepy_dataset.cache()\n",
    "print('First read of cached dataset')\n",
    "for d in cached:\n",
    "    print('Getting element', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second read of cached dataset\n",
      "Getting element tf.Tensor(0, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(1, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(2, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(3, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(4, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('Second read of cached dataset')\n",
    "for d in cached:\n",
    "    print('Getting element', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielding element 0\n",
      "Getting element tf.Tensor(0, shape=(), dtype=int32)\n",
      "Yielding element 1\n",
      "Yielding element 2\n",
      "Getting element tf.Tensor(2, shape=(), dtype=int32)\n",
      "Yielding element 3\n",
      "Yielding element 4\n",
      "Getting element tf.Tensor(4, shape=(), dtype=int32)\n",
      "Yielding element 5\n"
     ]
    }
   ],
   "source": [
    "sharded = sleepy_dataset.shard(2, 0)\n",
    "for d in sharded:\n",
    "    print('Getting element', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielding element 0\n",
      "Yielding element 1\n",
      "Yielding element 2\n",
      "Getting element tf.Tensor(1, shape=(), dtype=int32)\n",
      "Yielding element 3\n",
      "Getting element tf.Tensor(3, shape=(), dtype=int32)\n",
      "Yielding element 4\n",
      "Getting element tf.Tensor(4, shape=(), dtype=int32)\n",
      "Yielding element 5\n",
      "Getting element tf.Tensor(5, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(2, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "Reshuffled upon next iteration.\n",
      "Yielding element 0\n",
      "Yielding element 1\n",
      "Yielding element 2\n",
      "Getting element tf.Tensor(2, shape=(), dtype=int32)\n",
      "Yielding element 3\n",
      "Getting element tf.Tensor(3, shape=(), dtype=int32)\n",
      "Yielding element 4\n",
      "Getting element tf.Tensor(4, shape=(), dtype=int32)\n",
      "Yielding element 5\n",
      "Getting element tf.Tensor(0, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(5, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "shuffled = sleepy_dataset.shuffle(3)\n",
    "for d in shuffled:\n",
    "    print('Getting element', d)\n",
    "\n",
    "print('\\nReshuffled upon next iteration.')\n",
    "for d in shuffled:\n",
    "    print('Getting element', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tasks\n",
    "\n",
    "1. The `prefetch` method tries to fill a buffer with future data as soon as the first element has been requested. Write an example that uses the `prefetch` method. It expects a single argument: the buffer size for how many elements to process in advance. Do you see the behavior you expected? Why or why not? Try to insert a `time.sleep(2)` call in the `for`-loop to simulate an expensive neural network training step.\n",
    "1. What happens when you `shuffle` and then `cache` a dataset? You can use `repeat(<number>)` to iterate through the dataset more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielding element 0\n",
      "Yielding element 1\n",
      "Yielding element 2\n",
      "Getting element tf.Tensor(0, shape=(), dtype=int32)\n",
      "Yielding element 3\n",
      "Getting element tf.Tensor(1, shape=(), dtype=int32)\n",
      "Yielding element 4\n",
      "Getting element tf.Tensor(2, shape=(), dtype=int32)\n",
      "Yielding element 5\n",
      "Getting element tf.Tensor(3, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(4, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "prefetched = sleepy_dataset.prefetch(2)\n",
    "for d in prefetched:\n",
    "    # Prefetching cannot have an effect if we process each data point\n",
    "    # faster than the data pipeline is able to prefetch the next element.\n",
    "    # So if our calculations on the data are too fast, prefetching won't\n",
    "    # give us any improvement.\n",
    "    time.sleep(2)\n",
    "    print('Getting element', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielding element 0\n",
      "Yielding element 1\n",
      "Yielding element 2\n",
      "Yielding element 3\n",
      "Yielding element 4\n",
      "Yielding element 5\n",
      "Getting element tf.Tensor(3, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(0, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(4, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(5, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(2, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(1, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(3, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(0, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(4, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(5, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(2, shape=(), dtype=int32)\n",
      "Getting element tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "shuffled = sleepy_dataset.shuffle(6)\n",
    "shuffled_cached = shuffled.cache()\n",
    "for d in shuffled_cached.repeat(2):\n",
    "    print('Getting element', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the File System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some more perspective on different functions and what reading from individual files entails, we will now work an a subset of the Tiny-ImageNet-200 dataset. In total, there are 10&#8239;000 images.\n",
    "\n",
    "Below, you will see we created a list that contains file paths to all images ending in \".JPEG\" in a certain directory tree.\n",
    "You'll also notice that we used a simple for-loop to read, convert, and display the data in our \"dataset\". However, this way, we cannot take advantage of the advanced functionality of `tf.data` pipelines. You are going to change this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset contains only the validation set from Tiny ImageNet,\n",
    "# 10,000 images in total.\n",
    "dataset_path = Path('/p/project/training2306/datasets/tiny-tiny-imagenet').expanduser()\n",
    "assert dataset_path.exists(), 'dataset directory does not exist'\n",
    "\n",
    "# Create a generator containing all JPEG images in the dataset.\n",
    "files = dataset_path.glob('**/*.JPEG')\n",
    "# Convert from `Path` objects to strings\n",
    "files = list(map(str, files))\n",
    "\n",
    "boring_dataset = files\n",
    "\n",
    "for (i, path) in enumerate(boring_dataset):\n",
    "    print(path)\n",
    "\n",
    "    file_content = tf.io.read_file(path)\n",
    "    image = decode_jpeg(file_content)\n",
    "\n",
    "    display_image(image)\n",
    "\n",
    "    if i >= 5:\n",
    "        break\n",
    "\n",
    "# Instead of putting the operations in the above for-loop,\n",
    "# we could also have written something like this:\n",
    "\n",
    "# boring_dataset = map(tf.io.read_file, boring_dataset)\n",
    "# boring_dataset = map(decode_jpeg, boring_dataset)\n",
    "#\n",
    "# for image in itertools.islice(boring_dataset, 5):\n",
    "#     display_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TFDataset.from_tensor_slices(files)\n",
    "\n",
    "# Task: The dataset already contains the file paths;\n",
    "#       read the file contents here by mapping `tf.io.read_file`\n",
    "#       onto the dataset.\n",
    "dataset = dataset.map(tf.io.read_file)\n",
    "\n",
    "# Task: Now that you have the file contents, convert them\n",
    "#       to raw image data by mapping the function `decode_jpeg`.\n",
    "dataset = dataset.map(decode_jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you are done with the tasks, this should show you\n",
    "# 5 images like before!\n",
    "# Notice how slick this code looks compared to before? You'll see\n",
    "# what other advantages `tf.data.Dataset`s have in the rest of\n",
    "# this exercise.\n",
    "for image in dataset.take(5):\n",
    "    display_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Convert the iterative method we used to read and convert the data to use `tf.data.Dataset` methods. Similar to the Python built-in `map` function, the `map` method of `tf.data.Dataset`s takes a function to call on each element in the dataset, returning the element for the resulting dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "### Pipeline Profiling on Data in Individual Files\n",
    "\n",
    "To get some more perspective on different functions and what reading from individual files entails, you can now try to optimize a pre-built pipeline on the data you just wrote a pipeline yourself for.\n",
    "\n",
    "Change the below pipeline and run iterations to register the experiments. You can later display them in a table. Remember that we are reading from 10&#8239;000 different files, so try to see and understand how and why different functions in the pipeline change the runtime. We suggest starting by comparing the total runtime with `use_tensor_slices = False` vs. `use_tensor_slices = True` (see below). Afterwards, check out what happens below the \"Your pipeline starts here\" comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# Create a result collector.\n",
    "# It has some methods to format your results nicely\n",
    "# in tables.\n",
    "# Execute again to reset your results.\n",
    "fs_profiling_results = TFDatasetProfilingResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This dataset contains only the validation set from Tiny ImageNet,\n",
    "# 10,000 images in total.\n",
    "dataset_path = Path('/p/project/training2306/datasets/tiny-tiny-imagenet').expanduser()\n",
    "assert dataset_path.exists(), 'dataset directory does not exist'\n",
    "\n",
    "# Create a generator containing all JPEG images in the dataset.\n",
    "files = dataset_path.glob('**/*.JPEG')\n",
    "# Convert from `Path` objects to strings\n",
    "files = map(str, files)\n",
    "\n",
    "# Try changing this flag and see how the runtime performance changes.\n",
    "use_tensor_slices = False\n",
    "\n",
    "# Start a timer so we can measure starting from dataset creation.\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "if use_tensor_slices:\n",
    "    dataset = TFDataset.from_tensor_slices(list(files))\n",
    "else:\n",
    "    dataset = TFDataset.from_generator(\n",
    "        lambda: files,\n",
    "        output_signature=tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "    )\n",
    "\n",
    "# The cardinality is the _known_ length of the dataset.\n",
    "if dataset.cardinality() == tf.data.UNKNOWN_CARDINALITY:\n",
    "    print(\"TensorFlow does not know the dataset's cardinality.\")\n",
    "elif dataset.cardinality() == tf.data.INFINITE_CARDINALITY:\n",
    "    print('TensorFlow knows the dataset has infinite cardinality.')\n",
    "else:\n",
    "    print(f\"TensorFlow knows the dataset's cardinality is {dataset.cardinality()}.\")\n",
    "\n",
    "# Your pipeline starts here\n",
    "# =========================\n",
    "\n",
    "dataset = dataset.map(\n",
    "    tf.io.read_file,\n",
    "    \n",
    "    # TensorFlow can automatically parallelize some functions.\n",
    "    # To let TensorFlow decide based on the available CPU,\n",
    "    # use the value `tf.data.AUTOTUNE`.\n",
    "    # num_parallel_calls=4,\n",
    "\n",
    "    # When the function is deterministic, it will always give us\n",
    "    # the results in the same order. Being able to forgo this\n",
    "    # guarantee could increase our performance (only relevant if\n",
    "    # `num_parallel_calls` is set).\n",
    "    # deterministic=False,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda path: tf.io.decode_jpeg(path, channels=3),\n",
    "    # num_parallel_calls=4,\n",
    "    # deterministic=False,\n",
    ")\n",
    "\n",
    "# dataset = dataset.shard(8, 0)\n",
    "# dataset = dataset.shuffle(dataset.cardinality() if dataset.cardinality() > 0 else 1024)\n",
    "# dataset = dataset.prefetch(2048)\n",
    "\n",
    "iteration_start_time = time.perf_counter()\n",
    "\n",
    "# Iterate through the dataset\n",
    "for d in dataset:\n",
    "    pass\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "iteration_duration = end_time - iteration_start_time\n",
    "fs_profiling_results.add_result('tiny-tiny-imagenet', dataset, iteration_duration)\n",
    "\n",
    "print('\\nTotal duration:', duration, 'seconds')\n",
    "print('Iteration duration:', iteration_duration, 'seconds')\n",
    "print('Pipeline:\\n[' + ',\\n '.join(dataset.pipeline) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "##### Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(fs_profiling_results.to_html_table()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(fs_profiling_results.averages_to_html_table(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmup_iterations = 1\n",
    "\n",
    "display(HTML(\n",
    "    profiling_results.averages_to_html_table(num_warmup_iterations),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Format results as CSV\n",
    "\n",
    "In case you want to store your results in a standard format for text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fs_profiling_results.to_csv())"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "intro_scalable_dl_2023",
   "language": "python",
   "name": "intro_scalable_dl_2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
