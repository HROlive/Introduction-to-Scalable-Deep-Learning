- [./tutorials/day1/tutorial1/Tutorial1.ipynb](./tutorials/day1/tutorial1/Tutorial1.ipynb):
    - [PuTTY](https://www.chiark.greenend.org.uk/~sgtatham/putty/)
    - [Juwels access documentation pages](https://apps.fz-juelich.de/jsc/hps/juwels/access.html)
    - [JuDoor page](https://judoor.fz-juelich.de)
    - [course material Git reposity](https://gitlab.jsc.fz-juelich.de/MLDL_FZJ/juhaicu/jsc_public/sharedspace/teaching/intro_scalable_deep_learning/course-material.git)
    - [environment modules](https://modules.readthedocs.io/en/latest/index.html)
    - [`ml` command](https://modules.readthedocs.io/en/latest/ml.html)

- [./tutorials/day1/tutorial2/Tutorial2.ipynb](./tutorials/day1/tutorial2/Tutorial2.ipynb):
    - [_Message Passing Interface (MPI)_ standard](https://www.mpi-forum.org/)
    - [_mpi4py_](https://bitbucket.org/mpi4py/mpi4py)
    - [mpi4py documentation](https://mpi4py.readthedocs.io/en/stable/)
    - [NumPy](https://numpy.org/)
    - [Tiny-ImageNet-200](http://cs231n.stanford.edu/tiny-imagenet-200.zip)

- [./tutorials/day2/tutorial1/readme.md](./tutorials/day2/tutorial1/readme.md):
    - [Jupyter-JSC](https://jupyter-jsc.fz-juelich.de/)

- [./tutorials/day2/tutorial1/Day2_Tutorial1_DL_Basics_Recap.ipynb](./tutorials/day2/tutorial1/Day2_Tutorial1_DL_Basics_Recap.ipynb):
    - [LRFinder](https://github.com/WittmannF/LRFinder)

- [./tutorials/day2/tutorial1/utils/cifar10.py](./tutorials/day2/tutorial1/utils/cifar10.py):
    - [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html)

- [./tutorials/day2/tutorial1/solutions/cifar10.py](./tutorials/day2/tutorial1/solutions/cifar10.py):
    - [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html)

- [./tutorials/day2/tutorial2/cifar10.py](./tutorials/day2/tutorial2/cifar10.py):
    - [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html)

- [./tutorials/day2/tutorial2/Tutorial2.ipynb](./tutorials/day2/tutorial2/Tutorial2.ipynb):
    - [Horovod library](https://horovod.ai/)
    - [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)
    - [LMDB](https://symas.com/lmdb/)
    - [HDF5](https://www.hdfgroup.org/solutions/hdf5)
    - [Protocol Buffers](https://en.wikipedia.org/wiki/Protocol_Buffers)
    - [documentation on reading TFRecord files](https://www.tensorflow.org/tutorials/load_data/tfrecord#reading_a_tfrecord_file)
    - [_Horovod_](https://github.com/horovod/horovod)
    - [NCCL](https://developer.nvidia.com/nccl)
    - [list of Horovod-supported frameworks](https://github.com/horovod/horovod#supported-frameworks)
    - [Horovod basics module](https://github.com/horovod/horovod/blob/master/horovod/common/basics.py)

- [./tutorials/day2/tutorial2/solutions/cifar10.py](./tutorials/day2/tutorial2/solutions/cifar10.py):
    - [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html)

- [./tutorials/day3/tutorial2/Tutorial2.ipynb](./tutorials/day3/tutorial2/Tutorial2.ipynb):
    - [other methods in the `tf.data.Dataset` module](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)

- [./tutorials/day4/tutorial/Tutorial.ipynb](./tutorials/day4/tutorial/Tutorial.ipynb):
    - [You, Gitman, et al., _Large Batch Training of Convolutional Networks_](https://arxiv.org/abs/1708.03888)
    - [You, Li, et al., _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes_](https://arxiv.org/abs/1904.00962)
    - [Lin et al., _Don't Use Large Mini-Batches, Use Local SGD_](https://arxiv.org/abs/1808.07217)
    - [Goyal et al., _Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour_](https://arxiv.org/abs/1706.02677)
    - [Bottou et al., _Optimization Methods for Large-Scale Machine Learning_](https://arxiv.org/abs/1606.04838)
    - [Smith et al., _Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates_](https://arxiv.org/abs/1708.07120)
    - [Kingma et al., _Adam: A Method for Stochastic Optimization_](https://arxiv.org/abs/1412.6980)

- [./tutorials/day4/tutorial/lamb.py](./tutorials/day4/tutorial/lamb.py):
    - [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)

- [./tutorials/day4/tutorial/clr.py](./tutorials/day4/tutorial/clr.py):
    - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)
    - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)

- [./lectures/Day4/Lecture2/Slides/Slides.md](./lectures/Day4/Lecture2/Slides/Slides.md):
    - [Li et al., _Don't Use Large Mini-batches, Use Local SGD_, ICLR, 2020](https://openreview.net/forum?id=B1eyO1BFPr)

- [./tutorials/day5/references.md](./tutorials/day5/references.md):
    - [further references in this file](./tutorials/day5/references.md)
